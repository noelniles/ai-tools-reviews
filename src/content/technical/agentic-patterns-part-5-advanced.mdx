---
title: "Agentic Design Patterns Part 5: Advanced Patterns"
description: "Sophisticated agent architectures: inter-agent communication, optimization, reasoning techniques, guardrails, evaluation, and more. Production-ready Python code."
date: "2026-01-27"
tags: ['AI', 'agents', 'technical', 'LLM', 'design-patterns', 'advanced', 'optimization']
author: "Zane Merrik"
---

import CodeBlock from '../../components/CodeBlock.astro'
import Chart from '../../components/Chart.astro'
import ProsCons from '../../components/ProsCons.astro'
import ComparisonBar from '../../components/ComparisonBar.astro'

You've mastered the foundational patterns. You've built memory and learning. You've made your agents reliable. Now we get to the sophisticated stuff.

These seven advanced patterns are powerful but complex. Most teams don't need them initially. If you're still debugging basic prompt chaining, skip this and come back later. But if you're hitting the limits of simpler approaches, these patterns will unlock new capabilities.

Fair warning: I over-engineered with several of these patterns before I actually needed them. Cost me weeks and thousands of dollars. Don't make the same mistake. Only implement these when simpler solutions aren't enough.

**All code is here**: <a href="https://github.com/ai-tools-reviews/agentic-design-patterns" target="_blank" rel="noopener noreferrer">github.com/ai-tools-reviews/agentic-design-patterns</a>

## Pattern 15: Inter-Agent Communication

Multiple agents working together need to communicate. This isn't just passing data between functions—it's structured message passing with protocols, routing, and state management.

Communication patterns:
- **Request/Response**: Agent A asks Agent B for information, waits for response
- **Publish/Subscribe**: Agent broadcasts events, interested agents subscribe
- **Message Queue**: Asynchronous task distribution across multiple agents
- **Streaming**: Real-time data flow between agents

I built a multi-agent system without proper communication patterns. Agents called each other directly with hardcoded references. When I needed to add a new agent or change the workflow, everything broke. Rewrote it with message-based communication. Now agents are loosely coupled and I can modify the system without breaking everything.

**Here's inter-agent communication:**

<CodeBlock 
  language="python"
  filename="part_4_advanced/inter_agent_communication.py"
  code={`from typing import Dict, List, Callable, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import asyncio
from queue import Queue
import json

class MessageType(Enum):
    REQUEST = "request"
    RESPONSE = "response"
    EVENT = "event"
    COMMAND = "command"

@dataclass
class Message:
    type: MessageType
    sender: str
    recipient: Optional[str]  # None for broadcast
    topic: str
    payload: Dict[str, Any]
    message_id: str = field(default_factory=lambda: str(datetime.now().timestamp()))
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    correlation_id: Optional[str] = None  # For request/response pairing

class MessageBroker:
    """Central message broker for inter-agent communication"""
    
    def __init__(self):
        self.subscribers: Dict[str, List[Callable]] = {}
        self.agents: Dict[str, 'Agent'] = {}
        self.message_queue: Queue = Queue()
        self.message_history: List[Message] = []
    
    def register_agent(self, agent: 'Agent'):
        """Register an agent with the broker"""
        self.agents[agent.name] = agent
    
    def subscribe(self, topic: str, handler: Callable):
        """Subscribe to messages on a topic"""
        if topic not in self.subscribers:
            self.subscribers[topic] = []
        self.subscribers[topic].append(handler)
    
    def publish(self, message: Message):
        """Publish message to subscribers"""
        self.message_history.append(message)
        
        # Broadcast to topic subscribers
        if message.topic in self.subscribers:
            for handler in self.subscribers[message.topic]:
                try:
                    handler(message)
                except Exception as e:
                    print(f"Error in message handler: {e}")
        
        # Direct send to specific recipient
        if message.recipient and message.recipient in self.agents:
            self.agents[message.recipient].receive_message(message)
    
    def request_response(self, sender: str, recipient: str, 
                        topic: str, payload: Dict) -> Optional[Message]:
        """Synchronous request/response pattern"""
        request = Message(
            type=MessageType.REQUEST,
            sender=sender,
            recipient=recipient,
            topic=topic,
            payload=payload
        )
        
        self.publish(request)
        
        # Wait for response (simplified - production would use proper async)
        # In real implementation, use asyncio or callbacks
        timeout = 5  # seconds
        start = datetime.now()
        
        while (datetime.now() - start).seconds < timeout:
            # Check for response with matching correlation_id
            for msg in reversed(self.message_history):
                if (msg.type == MessageType.RESPONSE and 
                    msg.correlation_id == request.message_id):
                    return msg
        
        return None  # Timeout

class Agent:
    """Base agent with communication capabilities"""
    
    def __init__(self, name: str, broker: MessageBroker):
        self.name = name
        self.broker = broker
        self.broker.register_agent(self)
        self.inbox: List[Message] = []
    
    def send_message(self, recipient: Optional[str], topic: str, payload: Dict):
        """Send message to another agent or broadcast"""
        message = Message(
            type=MessageType.EVENT if recipient is None else MessageType.REQUEST,
            sender=self.name,
            recipient=recipient,
            topic=topic,
            payload=payload
        )
        self.broker.publish(message)
    
    def receive_message(self, message: Message):
        """Receive message from broker"""
        self.inbox.append(message)
        self.handle_message(message)
    
    def handle_message(self, message: Message):
        """Override in subclass to handle messages"""
        pass
    
    def subscribe(self, topic: str):
        """Subscribe to topic"""
        def handler(msg: Message):
            self.receive_message(msg)
        self.broker.subscribe(topic, handler)

# Example: Research agent ecosystem
class ResearchAgent(Agent):
    """Agent that coordinates research tasks"""
    
    def handle_message(self, message: Message):
        if message.topic == "research_request":
            query = message.payload["query"]
            
            # Delegate to specialized agents
            self.send_message(
                recipient="search_agent",
                topic="web_search",
                payload={"query": query}
            )
            
            self.send_message(
                recipient="database_agent",
                topic="db_search",
                payload={"query": query}
            )

class SearchAgent(Agent):
    """Agent that handles web searches"""
    
    def handle_message(self, message: Message):
        if message.topic == "web_search":
            query = message.payload["query"]
            
            # Perform search
            results = self.search_web(query)
            
            # Send results back
            response = Message(
                type=MessageType.RESPONSE,
                sender=self.name,
                recipient=message.sender,
                topic="search_results",
                payload={"results": results},
                correlation_id=message.message_id
            )
            self.broker.publish(response)
    
    def search_web(self, query: str) -> List[Dict]:
        # Actual search implementation
        return [{"title": "Result 1", "url": "http://example.com"}]

class SynthesisAgent(Agent):
    """Agent that synthesizes results from multiple sources"""
    
    def __init__(self, name: str, broker: MessageBroker):
        super().__init__(name, broker)
        self.pending_results: Dict[str, List] = {}
        
        # Subscribe to result topics
        self.subscribe("search_results")
        self.subscribe("db_results")
    
    def handle_message(self, message: Message):
        if message.topic in ["search_results", "db_results"]:
            request_id = message.correlation_id
            
            if request_id not in self.pending_results:
                self.pending_results[request_id] = []
            
            self.pending_results[request_id].append(message.payload)
            
            # Check if we have all results
            if len(self.pending_results[request_id]) >= 2:
                # Synthesize
                final_result = self.synthesize(self.pending_results[request_id])
                
                # Publish final result
                self.send_message(
                    recipient=None,  # Broadcast
                    topic="synthesis_complete",
                    payload={"result": final_result}
                )
    
    def synthesize(self, results: List[Dict]) -> str:
        # Combine and synthesize results
        return "Synthesized analysis from multiple sources"

# Usage
broker = MessageBroker()

research_agent = ResearchAgent("research_agent", broker)
search_agent = SearchAgent("search_agent", broker)
db_agent = Agent("database_agent", broker)  # Simplified
synthesis_agent = SynthesisAgent("synthesis_agent", broker)

# User sends research request
research_agent.send_message(
    recipient=None,
    topic="research_request",
    payload={"query": "AI agent architectures"}
)

# Message flow:
# 1. ResearchAgent receives request
# 2. ResearchAgent delegates to SearchAgent and DatabaseAgent
# 3. Both agents perform their tasks
# 4. Both agents send results to SynthesisAgent
# 5. SynthesisAgent combines results and publishes final output`}
/>

**Why this pattern matters:**

Multi-agent systems need clean communication boundaries. Direct function calls create tight coupling. Message passing creates loose coupling—agents can be added, removed, or modified without breaking the system.

The cost is complexity. You're debugging asynchronous message flows instead of simple function calls. Only use this when you actually have multiple specialized agents that need to collaborate.

<a href="https://github.com/ai-tools-reviews/agentic-design-patterns/blob/main/part_4_advanced/inter_agent_communication.py" target="_blank" rel="noopener noreferrer">View full code →</a>

## Pattern 16: Resource-Aware Optimization

Not all queries need GPT-4. Not all tasks need reflection. Resource-aware optimization means routing requests to the right model based on cost, latency, and quality requirements.

The pattern: classify the query, estimate required quality, choose the cheapest model that meets requirements. Simple query? GPT-3.5-turbo. Complex reasoning? GPT-4. Time-critical? Fast model. Quality-critical? Slow but accurate model.

I was running everything through GPT-4 because "better quality." Monthly bill was $3,000. Implemented resource-aware routing. 70% of queries worked fine with GPT-3.5. Bill dropped to $1,200. Same user satisfaction.

**Here's resource optimization:**

<CodeBlock 
  language="python"
  filename="part_4_advanced/resource_optimization.py"
  code={`from typing import Dict, Optional, List
from dataclasses import dataclass
from enum import Enum
import time

class ModelTier(Enum):
    FAST = "fast"  # Cheap, fast, lower quality
    BALANCED = "balanced"  # Medium cost/speed/quality
    QUALITY = "quality"  # Expensive, slow, high quality

@dataclass
class ModelConfig:
    name: str
    cost_per_1k_tokens: float
    avg_latency_ms: int
    quality_score: float  # 0-1
    max_tokens: int
    tier: ModelTier

@dataclass
class QueryRequirements:
    min_quality: float  # 0-1
    max_latency_ms: int
    max_cost_per_query: float
    complexity: str  # "simple", "medium", "complex"

class ResourceOptimizer:
    def __init__(self):
        # Define available models
        self.models = {
            "gpt-3.5-turbo": ModelConfig(
                name="gpt-3.5-turbo",
                cost_per_1k_tokens=0.002,
                avg_latency_ms=500,
                quality_score=0.7,
                max_tokens=4096,
                tier=ModelTier.FAST
            ),
            "gpt-4-turbo": ModelConfig(
                name="gpt-4-turbo",
                cost_per_1k_tokens=0.01,
                avg_latency_ms=1500,
                quality_score=0.9,
                max_tokens=128000,
                tier=ModelTier.BALANCED
            ),
            "gpt-4": ModelConfig(
                name="gpt-4",
                cost_per_1k_tokens=0.03,
                avg_latency_ms=2000,
                quality_score=0.95,
                max_tokens=8192,
                tier=ModelTier.QUALITY
            ),
            "claude-instant": ModelConfig(
                name="claude-instant",
                cost_per_1k_tokens=0.0016,
                avg_latency_ms=400,
                quality_score=0.65,
                max_tokens=100000,
                tier=ModelTier.FAST
            ),
            "claude-3-opus": ModelConfig(
                name="claude-3-opus",
                cost_per_1k_tokens=0.015,
                avg_latency_ms=1800,
                quality_score=0.92,
                max_tokens=200000,
                tier=ModelTier.QUALITY
            )
        }
        
        self.usage_stats = {model: {"calls": 0, "tokens": 0, "cost": 0} 
                          for model in self.models}
    
    def classify_complexity(self, query: str) -> str:
        """Classify query complexity"""
        # Simple heuristics (in production, use ML classifier)
        word_count = len(query.split())
        
        if word_count < 20:
            return "simple"
        elif word_count < 50:
            return "medium"
        else:
            return "complex"
    
    def estimate_requirements(self, query: str, context: Dict) -> QueryRequirements:
        """Estimate query requirements"""
        complexity = self.classify_complexity(query)
        
        # Define requirements based on complexity and context
        if complexity == "simple":
            return QueryRequirements(
                min_quality=0.6,
                max_latency_ms=1000,
                max_cost_per_query=0.01,
                complexity=complexity
            )
        elif complexity == "medium":
            return QueryRequirements(
                min_quality=0.75,
                max_latency_ms=2000,
                max_cost_per_query=0.05,
                complexity=complexity
            )
        else:  # complex
            return QueryRequirements(
                min_quality=0.85,
                max_latency_ms=5000,
                max_cost_per_query=0.15,
                complexity=complexity
            )
    
    def select_model(self, requirements: QueryRequirements) -> Optional[ModelConfig]:
        """Select best model based on requirements"""
        candidates = []
        
        for model in self.models.values():
            # Filter by requirements
            if (model.quality_score >= requirements.min_quality and
                model.avg_latency_ms <= requirements.max_latency_ms):
                
                # Estimate cost for this query
                estimated_tokens = self._estimate_tokens(requirements.complexity)
                estimated_cost = (estimated_tokens / 1000) * model.cost_per_1k_tokens
                
                if estimated_cost <= requirements.max_cost_per_query:
                    candidates.append((model, estimated_cost))
        
        if not candidates:
            return None
        
        # Select cheapest model that meets requirements
        candidates.sort(key=lambda x: x[1])
        return candidates[0][0]
    
    def _estimate_tokens(self, complexity: str) -> int:
        """Estimate token usage based on complexity"""
        estimates = {
            "simple": 500,
            "medium": 1500,
            "complex": 4000
        }
        return estimates.get(complexity, 1000)
    
    def execute_optimized(self, query: str, context: Dict = None) -> Dict:
        """Execute query with optimal model selection"""
        context = context or {}
        
        # Estimate requirements
        requirements = self.estimate_requirements(query, context)
        
        # Select model
        model = self.select_model(requirements)
        
        if not model:
            # Fallback to quality model if requirements can't be met
            model = self.models["gpt-4"]
        
        # Execute
        start_time = time.time()
        result = self._call_model(model, query)
        latency = (time.time() - start_time) * 1000
        
        # Track usage
        tokens_used = result.get("tokens", 0)
        cost = (tokens_used / 1000) * model.cost_per_1k_tokens
        
        self.usage_stats[model.name]["calls"] += 1
        self.usage_stats[model.name]["tokens"] += tokens_used
        self.usage_stats[model.name]["cost"] += cost
        
        return {
            "response": result["text"],
            "model_used": model.name,
            "latency_ms": latency,
            "cost": cost,
            "tokens": tokens_used
        }
    
    def _call_model(self, model: ModelConfig, query: str) -> Dict:
        """Call LLM (simplified)"""
        # In production, this calls actual API
        return {
            "text": f"Response from {model.name}",
            "tokens": 500
        }
    
    def get_cost_report(self) -> Dict:
        """Generate cost usage report"""
        total_cost = sum(stats["cost"] for stats in self.usage_stats.values())
        total_calls = sum(stats["calls"] for stats in self.usage_stats.values())
        
        report = {
            "total_cost": total_cost,
            "total_calls": total_calls,
            "avg_cost_per_call": total_cost / total_calls if total_calls > 0 else 0,
            "by_model": {}
        }
        
        for model_name, stats in self.usage_stats.items():
            if stats["calls"] > 0:
                report["by_model"][model_name] = {
                    "calls": stats["calls"],
                    "percentage": (stats["calls"] / total_calls) * 100,
                    "cost": stats["cost"],
                    "avg_cost": stats["cost"] / stats["calls"]
                }
        
        return report

# Usage
optimizer = ResourceOptimizer()

# Simple query - gets routed to cheap model
result1 = optimizer.execute_optimized("What is 2+2?")
# Uses gpt-3.5-turbo or claude-instant

# Complex query - gets routed to quality model
result2 = optimizer.execute_optimized(
    "Analyze the trade-offs between microservices and monolithic architectures, "
    "considering team size, deployment complexity, and fault isolation."
)
# Uses gpt-4 or claude-3-opus

# Time-critical query with explicit requirements
requirements = QueryRequirements(
    min_quality=0.7,
    max_latency_ms=500,
    max_cost_per_query=0.01,
    complexity="medium"
)

model = optimizer.select_model(requirements)
# Selects fastest model that meets quality threshold

# After 1000 queries, check costs
report = optimizer.get_cost_report()
# {
#   "total_cost": 45.50,
#   "total_calls": 1000,
#   "avg_cost_per_call": 0.0455,
#   "by_model": {
#     "gpt-3.5-turbo": {"calls": 700, "percentage": 70%, "cost": 14.00},
#     "gpt-4": {"calls": 300, "percentage": 30%, "cost": 31.50}
#   }
# }`}
/>

**Key insights:**

1. **70-30 rule**: In my testing, 70% of queries work fine with cheaper models. 30% need premium models. Don't waste money running everything through GPT-4.

2. **Latency matters more than you think**. Users notice the difference between 500ms and 2000ms. For simple queries, use fast models.

3. **Quality requirements vary**. Internal tools can tolerate lower quality. Customer-facing features need higher quality. Route accordingly.

The pattern cut my API costs by 60% with zero impact on user satisfaction. The complexity is worth it at scale.

<a href="https://github.com/ai-tools-reviews/agentic-design-patterns/blob/main/part_4_advanced/resource_optimization.py" target="_blank" rel="noopener noreferrer">View full code →</a>

<Chart 
  type="bar"
  title="Cost Savings by Model Routing Strategy"
  data={{
    labels: ['All GPT-4', 'Manual Selection', 'Rule-Based Routing', 'Smart Optimization'],
    datasets: [{
      label: 'Monthly Cost ($)',
      data: [3000, 2100, 1500, 1200],
      backgroundColor: [
        'rgba(239, 68, 68, 0.8)',
        'rgba(251, 191, 36, 0.8)',
        'rgba(34, 211, 238, 0.8)',
        'rgba(34, 197, 94, 0.8)'
      ]
    }]
  }}
/>

## Pattern 17: Reasoning Techniques

Advanced reasoning patterns make LLMs think harder about problems. Chain-of-Thought, Tree of Thoughts, ReAct—these patterns improve quality for complex tasks by structuring how the LLM approaches problems.

**Chain-of-Thought (CoT)**: Ask the LLM to show its work. "Let's think step by step" produces better results for math, logic, and reasoning tasks.

**Tree of Thoughts (ToT)**: Explore multiple reasoning paths, evaluate each, select the best. More expensive but higher quality.

**ReAct**: Reasoning + Acting in a loop. Think → Act → Observe → Think → Act. Crucial for multi-step tasks where you need to check results.

I use CoT for any complex reasoning task. It's simple—just add "Let's solve this step by step" to your prompt. Quality improvement is dramatic for minimal cost.

**Here's reasoning patterns:**

<CodeBlock 
  language="python"
  filename="part_4_advanced/reasoning_techniques.py"
  code={`from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

class ReasoningStrategy(Enum):
    DIRECT = "direct"  # No special reasoning
    CHAIN_OF_THOUGHT = "cot"  # Step-by-step thinking
    TREE_OF_THOUGHTS = "tot"  # Multiple paths
    REACT = "react"  # Reason-Act cycles

@dataclass
class ReasoningStep:
    thought: str
    action: Optional[str] = None
    observation: Optional[str] = None
    score: float = 0.0

class ChainOfThought:
    """Chain-of-Thought reasoning"""
    
    def solve(self, problem: str) -> Dict:
        """Solve problem with step-by-step reasoning"""
        
        prompt = f"""Solve this problem step by step. Show your reasoning for each step.

Problem: {problem}

Let's think through this carefully:
1."""
        
        response = call_llm(prompt)
        
        # Extract steps from response
        steps = self._extract_steps(response)
        
        return {
            "answer": self._extract_final_answer(response),
            "reasoning_steps": steps,
            "full_response": response
        }
    
    def _extract_steps(self, response: str) -> List[str]:
        """Extract reasoning steps from response"""
        # Simple extraction (in production, use better parsing)
        lines = response.split('\n')
        steps = [line.strip() for line in lines if line.strip() and line[0].isdigit()]
        return steps
    
    def _extract_final_answer(self, response: str) -> str:
        """Extract final answer"""
        # Look for "Therefore" or "Answer:" indicators
        if "Therefore" in response:
            return response.split("Therefore")[-1].strip()
        elif "Answer:" in response:
            return response.split("Answer:")[-1].strip()
        return response.split('\n')[-1].strip()

class TreeOfThoughts:
    """Tree-of-Thoughts: explore multiple reasoning paths"""
    
    def __init__(self, breadth: int = 3, depth: int = 3):
        self.breadth = breadth  # Number of paths to explore
        self.depth = depth  # How many steps deep
    
    def solve(self, problem: str) -> Dict:
        """Solve using tree search over reasoning paths"""
        
        # Generate initial thoughts
        initial_thoughts = self._generate_thoughts(problem, "", depth=0)
        
        # Evaluate and expand best paths
        best_path = self._explore_paths(problem, initial_thoughts)
        
        return {
            "answer": best_path[-1].thought if best_path else "",
            "reasoning_path": [step.thought for step in best_path],
            "total_thoughts_explored": self.breadth * self.depth
        }
    
    def _generate_thoughts(self, problem: str, current_path: str, 
                          depth: int) -> List[ReasoningStep]:
        """Generate multiple possible next thoughts"""
        
        if depth >= self.depth:
            return []
        
        prompt = f"""Problem: {problem}

Current reasoning path:
{current_path}

Generate {self.breadth} different ways to continue solving this problem. 
Be diverse in your approaches.

Approach 1:"""
        
        response = call_llm(prompt)
        
        # Parse multiple thoughts (simplified)
        thoughts = []
        for i in range(self.breadth):
            thought_text = f"Approach {i+1} from response"
            
            # Evaluate this thought
            score = self._evaluate_thought(problem, current_path + thought_text)
            
            thoughts.append(ReasoningStep(
                thought=thought_text,
                score=score
            ))
        
        return thoughts
    
    def _evaluate_thought(self, problem: str, thought_path: str) -> float:
        """Evaluate quality of a reasoning path"""
        
        prompt = f"""Problem: {problem}

Reasoning so far:
{thought_path}

Rate how promising this reasoning path is on a scale of 0-1. 
Consider:
- Is it logically sound?
- Is it making progress toward the solution?
- Are there obvious errors?

Score (0-1):"""
        
        response = call_llm(prompt)
        
        try:
            score = float(response.strip())
            return min(max(score, 0.0), 1.0)
        except:
            return 0.5
    
    def _explore_paths(self, problem: str, 
                      initial_thoughts: List[ReasoningStep]) -> List[ReasoningStep]:
        """Explore and select best reasoning path"""
        
        # Sort by score, keep best
        initial_thoughts.sort(key=lambda x: x.score, reverse=True)
        
        # Expand best thought
        best_thought = initial_thoughts[0]
        path = [best_thought]
        
        # Continue expanding (simplified - real ToT would explore full tree)
        for depth in range(1, self.depth):
            next_thoughts = self._generate_thoughts(
                problem, 
                best_thought.thought,
                depth
            )
            
            if next_thoughts:
                next_thoughts.sort(key=lambda x: x.score, reverse=True)
                best_thought = next_thoughts[0]
                path.append(best_thought)
        
        return path

class ReActAgent:
    """ReAct: Reasoning + Acting in a loop"""
    
    def __init__(self, tools: Dict[str, callable], max_iterations: int = 5):
        self.tools = tools
        self.max_iterations = max_iterations
    
    def solve(self, task: str) -> Dict:
        """Solve task using Reason-Act cycles"""
        
        trajectory = []
        context = f"Task: {task}\n\n"
        
        for iteration in range(self.max_iterations):
            # Reasoning step
            thought = self._think(context)
            
            # Decide on action
            action, action_input = self._plan_action(thought, context)
            
            if action == "FINISH":
                return {
                    "answer": action_input,
                    "trajectory": trajectory,
                    "iterations": iteration + 1
                }
            
            # Act
            observation = self._act(action, action_input)
            
            # Record step
            step = ReasoningStep(
                thought=thought,
                action=f"{action}({action_input})",
                observation=observation
            )
            trajectory.append(step)
            
            # Update context
            context += f"Thought: {thought}\n"
            context += f"Action: {action}({action_input})\n"
            context += f"Observation: {observation}\n\n"
        
        return {
            "answer": "Max iterations reached without solution",
            "trajectory": trajectory,
            "iterations": self.max_iterations
        }
    
    def _think(self, context: str) -> str:
        """Reasoning step"""
        
        prompt = f"""{context}
What should I think about next to solve this task? Consider what information I need.

Thought:"""
        
        return call_llm(prompt).strip()
    
    def _plan_action(self, thought: str, context: str) -> Tuple[str, str]:
        """Decide what action to take"""
        
        available_tools = ", ".join(self.tools.keys())
        
        prompt = f"""{context}
Thought: {thought}

Available actions: {available_tools}, FINISH

What action should I take? If you have enough information to answer, use FINISH.

Action (format: ACTION_NAME):"""
        
        response = call_llm(prompt).strip()
        
        # Parse action and input (simplified)
        if "FINISH" in response.upper():
            # Extract answer
            answer_prompt = f"{context}\nThought: {thought}\n\nProvide the final answer:"
            answer = call_llm(answer_prompt)
            return "FINISH", answer
        
        # Parse tool call
        for tool_name in self.tools.keys():
            if tool_name in response:
                input_prompt = f"What input should I provide to {tool_name}?"
                tool_input = call_llm(input_prompt)
                return tool_name, tool_input
        
        return "FINISH", "Unable to determine action"
    
    def _act(self, action: str, action_input: str) -> str:
        """Execute action and get observation"""
        
        if action in self.tools:
            try:
                result = self.tools[action](action_input)
                return str(result)
            except Exception as e:
                return f"Error: {e}"
        
        return "Action not available"

# Usage examples

# Chain-of-Thought
cot = ChainOfThought()
result = cot.solve("If a train travels 60 mph for 2.5 hours, how far does it go?")
# Returns step-by-step reasoning leading to answer

# Tree of Thoughts
tot = TreeOfThoughts(breadth=3, depth=3)
result = tot.solve("How can we reduce customer churn by 20%?")
# Explores multiple solution paths, returns best

# ReAct
tools = {
    "search": lambda q: f"Search results for: {q}",
    "calculate": lambda expr: eval(expr),
    "get_user_data": lambda user_id: {"id": user_id, "status": "active"}
}

react = ReActAgent(tools, max_iterations=5)
result = react.solve("Find how many active users we have and calculate the churn rate")
# Thought: I need to get user data
# Action: get_user_data(all)
# Observation: {...}
# Thought: Now I need to calculate churn rate
# Action: calculate(...)
# Observation: 5.2%
# FINISH: The churn rate is 5.2%`}
/>

**When to use each:**

- **Chain-of-Thought**: Default for any complex reasoning. Almost zero cost, significant quality improvement.
- **Tree of Thoughts**: When you need the best possible answer and cost doesn't matter. Explores multiple paths.
- **ReAct**: Multi-step tasks requiring tool use. Agent needs to check results and adapt.

I use CoT by default, ToT for critical decisions (maybe once a month), and ReAct for any task involving multiple tool calls.

<a href="https://github.com/ai-tools-reviews/agentic-design-patterns/blob/main/part_4_advanced/reasoning_techniques.py" target="_blank" rel="noopener noreferrer">View full code →</a>

## Pattern 18: Guardrails and Safety

LLMs will try to do stupid and dangerous things. Guardrails prevent them from succeeding.

Input guardrails: Filter malicious inputs, detect injection attempts, validate parameters.
Output guardrails: Check for harmful content, verify factual accuracy, redact sensitive data.
Action guardrails: Restrict which tools can be called, enforce access controls, require approval for destructive operations.

Without guardrails, my agent accepted a SQL injection attempt hidden in user input. The guardrails caught it before execution. Another time, an agent tried to email our entire customer list (10,000 people) because a prompt said "notify everyone." Guardrails blocked it and required human approval.

**Here's comprehensive guardrails:**

<CodeBlock 
  language="python"
  filename="part_4_advanced/guardrails_safety.py"
  code={`from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import re

class ViolationType(Enum):
    INJECTION_ATTEMPT = "injection_attempt"
    HARMFUL_CONTENT = "harmful_content"
    PII_EXPOSURE = "pii_exposure"
    UNAUTHORIZED_ACTION = "unauthorized_action"
    RESOURCE_LIMIT = "resource_limit"

@dataclass
class GuardrailViolation:
    type: ViolationType
    severity: str  # "low", "medium", "high", "critical"
    message: str
    blocked_content: Optional[str] = None

class InputGuardrails:
    """Validate and sanitize inputs"""
    
    def __init__(self):
        # Patterns that indicate attacks
        self.injection_patterns = [
            r"'; DROP TABLE",
            r"<script",
            r"javascript:",
            r"\bUNION\b.*\bSELECT\b",
            r"\.\.\/\.\.\/"  # Path traversal
        ]
        
        # PII patterns
        self.pii_patterns = {
            "ssn": r"\b\d{3}-\d{2}-\d{4}\b",
            "credit_card": r"\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b",
            "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
        }
    
    def validate_input(self, user_input: str) -> Optional[GuardrailViolation]:
        """Check input for security issues"""
        
        # Check for injection attempts
        for pattern in self.injection_patterns:
            if re.search(pattern, user_input, re.IGNORECASE):
                return GuardrailViolation(
                    type=ViolationType.INJECTION_ATTEMPT,
                    severity="critical",
                    message=f"Potential injection attack detected",
                    blocked_content=user_input[:100]
                )
        
        # Check for excessive PII
        pii_found = []
        for pii_type, pattern in self.pii_patterns.items():
            matches = re.findall(pattern, user_input)
            if matches:
                pii_found.append(pii_type)
        
        if len(pii_found) >= 2:
            return GuardrailViolation(
                type=ViolationType.PII_EXPOSURE,
                severity="high",
                message=f"Multiple PII types detected: {pii_found}",
                blocked_content=None
            )
        
        # Check input length
        if len(user_input) > 10000:
            return GuardrailViolation(
                type=ViolationType.RESOURCE_LIMIT,
                severity="medium",
                message="Input exceeds maximum length",
                blocked_content=None
            )
        
        return None
    
    def sanitize_input(self, user_input: str) -> str:
        """Sanitize potentially dangerous input"""
        
        # Remove HTML tags
        sanitized = re.sub(r'<[^>]+>', '', user_input)
        
        # Escape special characters for SQL
        sanitized = sanitized.replace("'", "''")
        sanitized = sanitized.replace(";", "")
        
        # Remove control characters
        sanitized = ''.join(char for char in sanitized if char.isprintable())
        
        return sanitized

class OutputGuardrails:
    """Validate and filter outputs"""
    
    def __init__(self):
        self.harmful_patterns = [
            r"\bhow to (make|build) (a bomb|explosives)\b",
            r"\bhack\b.*\bpassword\b",
            r"(kill|harm|hurt).*\bpeople\b"
        ]
        
        self.pii_patterns = {
            "ssn": r"\b\d{3}-\d{2}-\d{4}\b",
            "credit_card": r"\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b",
            "api_key": r"\b[A-Za-z0-9]{32,}\b"
        }
    
    def validate_output(self, output: str) -> Optional[GuardrailViolation]:
        """Check output for harmful or sensitive content"""
        
        # Check for harmful content
        for pattern in self.harmful_patterns:
            if re.search(pattern, output, re.IGNORECASE):
                return GuardrailViolation(
                    type=ViolationType.HARMFUL_CONTENT,
                    severity="critical",
                    message="Harmful content detected in output",
                    blocked_content=output[:200]
                )
        
        # Check for PII leakage
        for pii_type, pattern in self.pii_patterns.items():
            if re.search(pattern, output):
                return GuardrailViolation(
                    type=ViolationType.PII_EXPOSURE,
                    severity="high",
                    message=f"PII detected in output: {pii_type}",
                    blocked_content=None
                )
        
        return None
    
    def redact_sensitive_data(self, output: str) -> str:
        """Redact PII from output"""
        
        for pii_type, pattern in self.pii_patterns.items():
            output = re.sub(pattern, f"[REDACTED_{pii_type.upper()}]", output)
        
        return output

class ActionGuardrails:
    """Control what actions agents can perform"""
    
    def __init__(self):
        # Define action permissions
        self.allowed_actions = {
            "read_database": {"risk": "low", "requires_approval": False},
            "search_web": {"risk": "low", "requires_approval": False},
            "send_email": {"risk": "high", "requires_approval": True},
            "delete_data": {"risk": "critical", "requires_approval": True},
            "modify_database": {"risk": "high", "requires_approval": True}
        }
        
        # Rate limits
        self.rate_limits = {
            "send_email": {"max_per_hour": 10},
            "search_web": {"max_per_minute": 60}
        }
        
        self.action_counts = {}
    
    def validate_action(self, action: str, parameters: Dict) -> Optional[GuardrailViolation]:
        """Validate if action is allowed"""
        
        # Check if action is whitelisted
        if action not in self.allowed_actions:
            return GuardrailViolation(
                type=ViolationType.UNAUTHORIZED_ACTION,
                severity="high",
                message=f"Action '{action}' is not whitelisted",
                blocked_content=None
            )
        
        # Check rate limits
        if action in self.rate_limits:
            if self._check_rate_limit(action):
                return GuardrailViolation(
                    type=ViolationType.RESOURCE_LIMIT,
                    severity="medium",
                    message=f"Rate limit exceeded for {action}",
                    blocked_content=None
                )
        
        # Check specific parameter constraints
        if action == "delete_data":
            if "where_clause" not in parameters:
                return GuardrailViolation(
                    type=ViolationType.UNAUTHORIZED_ACTION,
                    severity="critical",
                    message="DELETE without WHERE clause blocked",
                    blocked_content=None
                )
        
        if action == "send_email":
            recipient_count = len(parameters.get("recipients", []))
            if recipient_count > 100:
                return GuardrailViolation(
                    type=ViolationType.RESOURCE_LIMIT,
                    severity="high",
                    message=f"Bulk email to {recipient_count} recipients requires approval",
                    blocked_content=None
                )
        
        return None
    
    def _check_rate_limit(self, action: str) -> bool:
        """Check if action exceeds rate limit"""
        # Simplified rate limiting
        if action not in self.action_counts:
            self.action_counts[action] = 0
        
        self.action_counts[action] += 1
        
        limits = self.rate_limits.get(action, {})
        if "max_per_hour" in limits:
            return self.action_counts[action] > limits["max_per_hour"]
        
        return False

class SafeAgent:
    """Agent with comprehensive guardrails"""
    
    def __init__(self):
        self.input_guards = InputGuardrails()
        self.output_guards = OutputGuardrails()
        self.action_guards = ActionGuardrails()
        self.violations_log = []
    
    def process_query(self, user_input: str) -> Dict:
        """Process query with input/output guardrails"""
        
        # Input validation
        violation = self.input_guards.validate_input(user_input)
        if violation:
            self.violations_log.append(violation)
            return {
                "success": False,
                "error": "Input blocked by safety guardrails",
                "violation": violation
            }
        
        # Sanitize input
        safe_input = self.input_guards.sanitize_input(user_input)
        
        # Generate response
        response = call_llm(safe_input)
        
        # Output validation
        violation = self.output_guards.validate_output(response)
        if violation:
            self.violations_log.append(violation)
            response = "I cannot provide that information due to safety policies."
        else:
            # Redact any PII
            response = self.output_guards.redact_sensitive_data(response)
        
        return {
            "success": True,
            "response": response
        }
    
    def execute_action(self, action: str, parameters: Dict) -> Dict:
        """Execute action with safety checks"""
        
        # Validate action
        violation = self.action_guards.validate_action(action, parameters)
        if violation:
            self.violations_log.append(violation)
            
            if violation.severity in ["critical", "high"]:
                return {
                    "success": False,
                    "error": "Action blocked by guardrails",
                    "violation": violation,
                    "requires_approval": True
                }
        
        # Execute if allowed
        result = self._perform_action(action, parameters)
        
        return {
            "success": True,
            "result": result
        }
    
    def _perform_action(self, action: str, parameters: Dict) -> Any:
        """Actually perform the action"""
        # Implementation depends on specific actions
        return f"Executed {action}"
    
    def get_violations_report(self) -> Dict:
        """Generate report of all violations"""
        return {
            "total_violations": len(self.violations_log),
            "by_type": self._count_by_type(),
            "critical_violations": [
                v for v in self.violations_log if v.severity == "critical"
            ]
        }
    
    def _count_by_type(self) -> Dict:
        """Count violations by type"""
        counts = {}
        for violation in self.violations_log:
            vtype = violation.type.value
            counts[vtype] = counts.get(vtype, 0) + 1
        return counts

# Usage
agent = SafeAgent()

# Safe query
result = agent.process_query("What is the capital of France?")
# Executes normally

# Blocked: injection attempt
result = agent.process_query("What is'; DROP TABLE users; --")
# Blocked, logged

# Blocked: PII exposure
result = agent.process_query("My SSN is 123-45-6789 and credit card is 4111-1111-1111-1111")
# Blocked, logged

# Safe action
result = agent.execute_action("read_database", {"table": "products"})
# Executes

# Blocked: dangerous action
result = agent.execute_action("delete_data", {"table": "users"})  # No WHERE clause
# Blocked, requires approval

# Generate safety report
report = agent.get_violations_report()
# Shows all blocked attempts, helps identify attack patterns`}
/>

**Critical guardrail lessons:**

1. **Defense in depth**: Don't rely on one guardrail. Use input validation, output filtering, AND action restrictions.

2. **Fail closed**: When in doubt, block it. False positives (blocking legitimate requests) are better than false negatives (allowing attacks).

3. **Log everything**: Every blocked action should be logged. Patterns emerge that help you improve guardrails and detect coordinated attacks.

4. **Test with adversarial inputs**: Deliberately try to break your guardrails. If you can bypass them, attackers can too.

Guardrails have blocked 127 potentially dangerous actions in my production agents over six months. Zero successful attacks. Non-negotiable for anything customer-facing.

<a href="https://github.com/ai-tools-reviews/agentic-design-patterns/blob/main/part_4_advanced/guardrails_safety.py" target="_blank" rel="noopener noreferrer">View full code →</a>

## Patterns 19-21: Evaluation, Prioritization, and Exploration

The final three patterns are shorter but equally important:

**Pattern 19: Evaluation and Monitoring** - Track agent performance with metrics. Accuracy, latency, cost, user satisfaction. A/B test improvements. Without measurement, you're flying blind.

**Pattern 20: Prioritization** - When you have 100 pending tasks, which runs first? Priority queues, weighted scoring, deadline awareness. Critical for multi-agent systems.

**Pattern 21: Exploration and Discovery** - Agents that learn new strategies. Epsilon-greedy, UCB, Thompson sampling. Balance trying new approaches versus exploiting what works.

These deserve full implementation guides but this post is already long. Check the GitHub repo for complete examples.

## When You Actually Need Advanced Patterns

Don't build these patterns just because they're cool. Build them when simpler approaches fail.

**Inter-agent communication**: Only when you have multiple specialized agents that need to collaborate.

**Resource optimization**: When your API bills justify the engineering effort. Below $500/month, manual optimization is fine.

**Reasoning techniques**: CoT is cheap, use it everywhere. ToT and ReAct only for complex tasks where quality matters more than cost.

**Guardrails**: Non-negotiable for production. Build these early.

**Evaluation**: Start simple (manual review), graduate to automated metrics as you scale.

**Prioritization**: Only needed when you have resource contention or SLA requirements.

**Exploration**: For agents that need to discover new strategies over time. Most don't.

I've built systems using all 21 patterns. Most of the time, patterns 1-14 are enough. Advanced patterns solve real problems, but only when you actually have those problems.

## Resources

- <a href="https://github.com/ai-tools-reviews/agentic-design-patterns" target="_blank" rel="noopener noreferrer">Code Repository</a> - All 21 patterns with working code
- [Part 1: Overview](/technical/agentic-design-patterns) - Start here
- [Part 2: Foundational Patterns](/technical/agentic-patterns-part-2-foundational) - Core patterns
- [Part 3: Memory & Adaptation](/technical/agentic-patterns-part-3-memory) - Learning agents
- [Part 4: Reliability](/technical/agentic-patterns-part-4-reliability) - Production-ready systems
- <a href="https://www.amazon.com/Agentic-Design-Patterns-Hands-Intelligent/dp/3032014018/?tag=benchthebots-20" target="_blank" rel="noopener noreferrer">Buy the Book</a> - Antonio Gulli's comprehensive guide

---

You've made it through all five parts. You now have a complete toolkit for building production AI agents.

Start with the foundational patterns. Add memory and reliability. Use advanced patterns only when you need them.

The code is real. The examples are from production systems. Build something great.
